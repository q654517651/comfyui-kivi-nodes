"""
Loop Detection and Extraction Node for ComfyUI
è‡ªåŠ¨æ£€æµ‹è§†é¢‘ä¸­çš„å¾ªç¯æ¨¡å¼å¹¶æå–å¾ªç¯ç‰‡æ®µåŠå…¶å‰åå¸§
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional, Dict, Any


class LoopDetectExtract:
    """
    æ£€æµ‹è§†é¢‘ä¸­çš„å¾ªç¯æ¨¡å¼å¹¶æå–å¾ªç¯ç‰‡æ®µ
    
    ä½¿ç”¨ FFT è‡ªç›¸å…³åˆ†ææ£€æµ‹å‘¨æœŸæ€§ï¼Œç„¶åç²¾ç¡®å®šä½å¾ªç¯è¾¹ç•Œ
    """
    
    DESCRIPTION = "è‡ªåŠ¨æ£€æµ‹è§†é¢‘å¸§åºåˆ—ä¸­çš„å¾ªç¯æ¨¡å¼ï¼Œæå–å¾ªç¯ç‰‡æ®µã€‚ä½¿ç”¨ GPU åŠ é€Ÿçš„ FFT è‡ªç›¸å…³åˆ†æå’Œç›¸ä¼¼åº¦åŒ¹é…ã€‚"
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "frames": (
                    "IMAGE",
                    {
                        "tooltip": "è¾“å…¥å›¾åƒåºåˆ— (batch, height, width, channels)ï¼Œå€¼èŒƒå›´ [0, 1]"
                    }
                ),
                "confidence_threshold": (
                    "FLOAT",
                    {
                        "default": 0.55,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.05,
                        "tooltip": "å¾ªç¯æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è¿”å›å…¨éƒ¨å¸§"
                    }
                ),
            },
            "optional": {
                "min_period": (
                    "INT",
                    {
                        "default": 24,
                        "min": 2,
                        "max": 1000,
                        "step": 1,
                        "tooltip": "æœ€å°å¾ªç¯å‘¨æœŸï¼ˆå¸§æ•°ï¼‰"
                    }
                ),
                "max_period": (
                    "INT",
                    {
                        "default": 300,
                        "min": 4,
                        "max": 5000,
                        "step": 1,
                        "tooltip": "æœ€å¤§å¾ªç¯å‘¨æœŸï¼ˆå¸§æ•°ï¼‰"
                    }
                ),
                "analysis_stride": (
                    "INT",
                    {
                        "default": 2,
                        "min": 1,
                        "max": 8,
                        "step": 1,
                        "tooltip": "åˆ†ææ—¶çš„å¸§é‡‡æ ·æ­¥é•¿ï¼Œè¶Šå¤§è¶Šå¿«ä½†ç²¾åº¦é™ä½"
                    }
                ),
                "analysis_size": (
                    "INT",
                    {
                        "default": 256,
                        "min": 64,
                        "max": 512,
                        "step": 32,
                        "tooltip": "åˆ†ææ—¶å›¾åƒç¼©æ”¾çš„ç›®æ ‡å°ºå¯¸"
                    }
                ),
                "seam_threshold": (
                    "FLOAT",
                    {
                        "default": 0.85,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "é¦–å°¾æ¥ç¼ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡åˆ™ä¼˜å…ˆåˆ¤å®šæ•´æ®µé—­ç¯"
                    }
                ),
                "min_pairs": (
                    "INT",
                    {
                        "default": 12,
                        "min": 4,
                        "max": 200,
                        "step": 1,
                        "tooltip": "å€™é€‰å‘¨æœŸéœ€æ»¡è¶³çš„æœ€å°æˆå¯¹æ ·æœ¬æ•°"
                    }
                ),
                "prefer_longer_cycles": (
                    "BOOLEAN",
                    {
                        "default": True,
                        "tooltip": "åœ¨åˆ†æ•°æ¥è¿‘æ—¶å€¾å‘é€‰æ‹©æ›´é•¿å‘¨æœŸ"
                    }
                ),
                "length_bias": (
                    "FLOAT",
                    {
                        "default": 0.05,
                        "min": 0.0,
                        "max": 0.2,
                        "step": 0.01,
                        "tooltip": "å¯¹è¾ƒé•¿å‘¨æœŸçš„è½»åº¦åå¥½å¼ºåº¦"
                    }
                ),
                "motion_weight": (
                    "FLOAT",
                    {
                        "default": 0.25,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.05,
                        "tooltip": "å°†è¿åŠ¨å‘¨æœŸæ€§å¼•å…¥å€™é€‰è¯„åˆ†çš„æƒé‡"
                    }
                ),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "INT", "INT", "FLOAT", "STRING")
    RETURN_NAMES = (
        "loop_frames",
        "loop_start",
        "loop_period",
        "confidence",
        "report"
    )
    FUNCTION = "detect_and_extract"
    CATEGORY = "kivi_nodes"
    
    def detect_and_extract(
        self,
        frames: torch.Tensor,
        confidence_threshold: float,
        min_period: int = 24,
        max_period: int = 300,
        analysis_stride: int = 2,
        analysis_size: int = 256,
        seam_threshold: float = 0.85,
        min_pairs: int = 12,
        prefer_longer_cycles: bool = True,
        length_bias: float = 0.05,
        motion_weight: float = 0.25,
    ) -> Tuple:
        """
        æ£€æµ‹å¾ªç¯å¹¶æå–å¾ªç¯å¸§
        
        Returns:
            (loop_frames, loop_start, loop_period, confidence, report)
        """
        device = self._get_device(frames)
        N = frames.shape[0]
        
        # è¾¹ç•Œæ£€æŸ¥
        if N < min_period + 1:
            return self._return_all_frames(frames, f"å¸§æ•°å¤ªå°‘ ({N} < {min_period+1})")
        
        try:
            # 1. é¢„å¤„ç†ï¼šé‡‡æ ·å’Œç¼©æ”¾
            frames_analysis = self._prepare_for_analysis(
                frames, stride=analysis_stride, target_size=analysis_size, device=device
            )
            
            # 2. ç‰¹å¾æå–
            features = self._extract_features(frames_analysis, device=device)
            
            # 2.1 ä¸‹é‡‡æ ·ç°åº¦ï¼ˆç”¨äº seam å’Œè¿åŠ¨åˆ†æï¼‰
            gray_small = self._downscaled_gray(frames, device=device, size=64)
            
            # 2.2 æ¥ç¼é¢„æ£€ï¼ˆæ•´æ®µé—­ç¯æ£€æµ‹ï¼‰
            seam_sim = self._seam_score(gray_small, window=3)
            
            # 2.3 è¿åŠ¨å‘¨æœŸæ€§ï¼ˆç”¨äºå€™é€‰è¯„åˆ†çš„åŠ æƒï¼‰
            motion_ac = self._motion_autocorr(gray_small)  # (N-1,)
            
            # 3. FFT è‡ªç›¸å…³åˆ†æ
            autocorr = self._compute_autocorrelation(features)
            
            # 4. æ£€æµ‹å¾ªç¯å‘¨æœŸï¼ˆä½¿ç”¨å¢å¼ºçš„å¤šå› ç´ è¯„åˆ†ï¼‰
            period_stride, confidence, candidates = self._detect_period(
                autocorr,
                N=len(frames_analysis),  # ä¼ å…¥åˆ†æå¸§æ•°ç”¨äºæ ·æœ¬å¯¹æƒ©ç½š
                min_period=max(1, min_period // analysis_stride),
                max_period=min(len(autocorr) - 1, max_period // analysis_stride),
                motion_ac=motion_ac[:len(autocorr)] if motion_ac is not None else None,
                min_pairs=min_pairs,
                prefer_longer=prefer_longer_cycles,
                length_bias=length_bias,
                motion_weight=motion_weight,
            )
            
            # æ˜ å°„å›åŸå§‹å¸§ç©ºé—´
            period = period_stride * analysis_stride
            
            # 4.5 ä¼˜å…ˆå¤„ç†"æ•´æ®µé—­ç¯"åœºæ™¯ï¼ˆseam é¢„æ£€ï¼‰
            if confidence < confidence_threshold and seam_sim >= seam_threshold:
                # å†…éƒ¨æ²¡æœ‰è¶³å¤Ÿå¼ºå³°ï¼Œä½†é¦–å°¾æ¥ç¼ç›¸ä¼¼åº¦å¾ˆé«˜ â†’ åˆ¤å®šæ•´æ®µä¸ºä¸€ä¸ªå¾ªç¯
                loop_start = 0
                period = N
                loop_frames = self._extract_frames(frames, loop_start, period)
                report = self._generate_report(
                    N, loop_start, period, float(seam_sim),
                    [{"period": N, "score": float(seam_sim), "prominence": float(seam_sim)}]
                )
                report = f"[æ•´æ®µé—­ç¯æ£€æµ‹] æ¥ç¼ç›¸ä¼¼åº¦: {seam_sim:.3f}\n" + report
                return (
                    loop_frames,
                    0, int(period), float(seam_sim), report
                )
            
            # 5. æ£€æŸ¥ç½®ä¿¡åº¦
            if confidence < confidence_threshold:
                return self._return_all_frames(
                    frames, 
                    f"ç½®ä¿¡åº¦ä¸è¶³ ({confidence:.3f} < {confidence_threshold})"
                )
            
            # 6. ç›¸ä½é”å®šï¼šæ‰¾åˆ°æœ€ä½³èµ·å§‹ç‚¹
            loop_start = self._find_best_start(frames, period, device=device)
            
            # 7. è¾¹ç•Œç²¾ä¿®
            loop_start, period = self._refine_boundaries(
                frames, loop_start, period, device=device
            )
            
            # 8. æå–å¾ªç¯å¸§
            loop_frames = self._extract_frames(frames, loop_start, period)
            
            # 9. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(
                N, loop_start, period, confidence, candidates
            )
            
            return (
                loop_frames,
                int(loop_start),
                int(period),
                float(confidence),
                report
            )
            
        except Exception as e:
            print(f"[LoopDetectExtract] é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._return_all_frames(frames, f"å¤„ç†å‡ºé”™: {str(e)}")
        finally:
            # æ¸…ç† GPU å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def _get_device(self, tensor: torch.Tensor) -> torch.device:
        """è·å–åˆé€‚çš„è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        return tensor.device
    
    def _downscaled_gray(self, frames: torch.Tensor, device: torch.device, size: int = 64) -> torch.Tensor:
        """
        ä¸‹é‡‡æ ·ç°åº¦å›¾åƒï¼ˆç”¨äº seam æ£€æµ‹å’Œè¿åŠ¨åˆ†æï¼‰
        
        Args:
            frames: è¾“å…¥å¸§ (N, H, W, C)
            device: è®¡ç®—è®¾å¤‡
            size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            ç°åº¦å›¾åƒ (N, size, size)
        """
        frames = frames.to(device)
        if frames.shape[-1] >= 3:
            gray = (0.299 * frames[..., 0] + 0.587 * frames[..., 1] + 0.114 * frames[..., 2])
        else:
            gray = frames[..., 0]
        
        gray_small = F.interpolate(
            gray.unsqueeze(1), 
            size=(size, size), 
            mode='bilinear', 
            align_corners=False
        ).squeeze(1)
        
        return gray_small  # (N, size, size)
    
    def _seam_score(self, gray_small: torch.Tensor, window: int = 3) -> float:
        """
        è®¡ç®—é¦–å°¾æ¥ç¼ç›¸ä¼¼åº¦ï¼ˆç”¨äºæ£€æµ‹æ•´æ®µå¾ªç¯ï¼‰
        
        Args:
            gray_small: ç°åº¦å›¾åƒ (N, H, W)
            window: æ¯”è¾ƒçš„å¸§çª—å£å¤§å°
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° [0, 1]ï¼Œè¶Šé«˜è¡¨ç¤ºé¦–å°¾è¶Šç›¸ä¼¼
        """
        N = gray_small.shape[0]
        w = min(window, N // 2)
        if w <= 0:
            return 0.0
        
        # æ¯”è¾ƒæœ€å w å¸§å’Œæœ€å‰ w å¸§
        last_frames = gray_small[N - w: N]   # æœ€å w å¸§
        first_frames = gray_small[0: w]      # æœ€å‰ w å¸§
        
        # ç”¨ L1 è·ç¦»è®¡ç®—å·®å¼‚
        diff = (last_frames - first_frames).abs().mean()
        
        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆç°åº¦èŒƒå›´ [0, 1]ï¼‰
        similarity = float(torch.clamp(1.0 - diff, 0.0, 1.0))
        
        return similarity
    
    def _motion_autocorr(self, gray_small: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—è¿åŠ¨çš„è‡ªç›¸å…³ï¼ˆåŸºäºå¸§å·®èƒ½é‡ï¼‰
        
        ç”¨äºæŠ‘åˆ¶"å‡ ä¹æ²¡åŠ¨"çš„å¾®å¾ªç¯ï¼Œå¢å¼ºæœ‰è¿åŠ¨çš„çœŸå®å¾ªç¯
        
        Args:
            gray_small: ç°åº¦å›¾åƒ (N, H, W)
            
        Returns:
            è¿åŠ¨è‡ªç›¸å…³åºåˆ— (N-1,)
        """
        # è®¡ç®—ä¸€é˜¶å¸§å·®çš„èƒ½é‡åºåˆ—
        frame_diff = (gray_small[1:] - gray_small[:-1]).abs().mean(dim=(1, 2))  # (N-1,)
        
        # é›¶å‡å€¼åŒ–
        d = (frame_diff - frame_diff.mean()) / (frame_diff.std() + 1e-6)
        
        M = d.shape[0]
        
        # ä½¿ç”¨ FFT è®¡ç®—è‡ªç›¸å…³
        # è¡¥é›¶åˆ° 2M ç¡®ä¿çº¿æ€§ç›¸å…³
        fft_result = torch.fft.rfft(d, n=2 * M)
        power_spectrum = (fft_result * fft_result.conj()).real
        autocorr = torch.fft.irfft(power_spectrum, n=2 * M)[:M]
        
        # å½’ä¸€åŒ–
        autocorr = autocorr / (autocorr[0] + 1e-9)
        autocorr[0] = 0.0  # å¿½ç•¥é›¶å»¶è¿Ÿ
        
        return autocorr  # (M,)
    
    def _prepare_for_analysis(
        self, 
        frames: torch.Tensor, 
        stride: int, 
        target_size: int,
        device: torch.device
    ) -> torch.Tensor:
        """é¢„å¤„ç†ï¼šé‡‡æ ·å’Œç¼©æ”¾"""
        # å…ˆå°† frames ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ï¼Œé¿å…è®¾å¤‡ä¸åŒ¹é…é”™è¯¯
        frames = frames.to(device)
        
        # é‡‡æ ·
        indices = torch.arange(0, frames.shape[0], stride, device=device)
        sampled = frames.index_select(0, indices)
        
        # ç¼©æ”¾ (NHWC -> NCHW -> resize -> NCHW)
        B, H, W, C = sampled.shape
        scale = target_size / max(H, W)
        new_h = max(1, int(H * scale))
        new_w = max(1, int(W * scale))
        
        # è½¬æ¢ä¸º NCHW
        sampled_nchw = sampled.permute(0, 3, 1, 2)
        
        # è°ƒæ•´å¤§å°
        resized = F.interpolate(
            sampled_nchw,
            size=(new_h, new_w),
            mode='bilinear',
            align_corners=False
        )
        
        # è½¬å› NHWC
        return resized.permute(0, 2, 3, 1)
    
    def _extract_features(
        self, 
        frames: torch.Tensor, 
        device: torch.device
    ) -> torch.Tensor:
        """
        æå–æ··åˆç‰¹å¾ï¼šç°åº¦å— + é¢œè‰²ç›´æ–¹å›¾ + è¾¹ç¼˜ç‰¹å¾
        è¿”å› (N, D) çš„ç‰¹å¾çŸ©é˜µ
        """
        N, H, W, C = frames.shape
        features_list = []
        
        # è½¬æ¢ä¸º NCHW ä¾¿äºå¤„ç†
        frames_nchw = frames.permute(0, 3, 1, 2)
        
        # 1. ç°åº¦ç‰¹å¾ (16x16 ç½‘æ ¼)
        if C >= 3:
            gray = 0.299 * frames_nchw[:, 0] + 0.587 * frames_nchw[:, 1] + 0.114 * frames_nchw[:, 2]
        else:
            gray = frames_nchw[:, 0]
        
        gray = gray.unsqueeze(1)  # (N, 1, H, W)
        gray_pooled = F.adaptive_avg_pool2d(gray, (16, 16))  # (N, 1, 16, 16)
        gray_feat = gray_pooled.reshape(N, -1)  # (N, 256)
        features_list.append(gray_feat)
        
        # 2. é¢œè‰²ç›´æ–¹å›¾ç‰¹å¾ (ç®€åŒ–ç‰ˆ)
        if C >= 3:
            # å¯¹æ¯ä¸ªé¢œè‰²é€šé“è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            color_mean = frames_nchw.mean(dim=(2, 3))  # (N, C)
            color_std = frames_nchw.std(dim=(2, 3))    # (N, C)
            color_feat = torch.cat([color_mean, color_std], dim=1)  # (N, 2C)
            features_list.append(color_feat)
        
        # 3. è¾¹ç¼˜ç‰¹å¾ (Sobel)
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                               dtype=frames.dtype, device=device).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                               dtype=frames.dtype, device=device).view(1, 1, 3, 3)
        
        edges_x = F.conv2d(gray, sobel_x, padding=1)
        edges_y = F.conv2d(gray, sobel_y, padding=1)
        edges = torch.sqrt(edges_x ** 2 + edges_y ** 2 + 1e-8)
        
        edge_mean = edges.mean(dim=(1, 2, 3))  # (N,)
        edge_std = edges.std(dim=(1, 2, 3))    # (N,)
        edge_feat = torch.stack([edge_mean, edge_std], dim=1)  # (N, 2)
        features_list.append(edge_feat)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        features = torch.cat(features_list, dim=1)  # (N, D)
        
        # L2 å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1)
        
        return features
    
    def _compute_autocorrelation(self, features: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨ FFT è®¡ç®—è‡ªç›¸å…³
        è¿”å›å½’ä¸€åŒ–çš„è‡ªç›¸å…³åºåˆ—
        """
        N, D = features.shape
        
        # é›¶å‡å€¼åŒ–
        features_centered = features - features.mean(dim=0, keepdim=True)
        
        # è½¬ç½®ä»¥ä¾¿å¯¹æ¯ä¸ªç»´åº¦åš FFT
        features_t = features_centered.T  # (D, N)
        
        # FFT -> åŠŸç‡è°± -> IFFT
        fft_result = torch.fft.rfft(features_t, n=2*N, dim=1)
        power_spectrum = (fft_result * fft_result.conj()).real
        autocorr_per_dim = torch.fft.irfft(power_spectrum, n=2*N, dim=1)
        
        # åœ¨ç»´åº¦ä¸Šæ±‚å’Œ
        autocorr = autocorr_per_dim.sum(dim=0)[:N]  # åªå–å‰ N ä¸ª
        
        # å½’ä¸€åŒ–åˆ° [0, 1]
        autocorr = autocorr / (autocorr[0] + 1e-9)
        autocorr[0] = 0.0  # å¿½ç•¥é›¶å»¶è¿Ÿ
        
        return autocorr
    
    def _detect_period(
        self,
        autocorr: torch.Tensor,
        N: int,
        min_period: int,
        max_period: int,
        motion_ac: Optional[torch.Tensor] = None,
        min_pairs: int = 12,
        prefer_longer: bool = True,
        length_bias: float = 0.05,
        motion_weight: float = 0.25,
    ) -> Tuple[int, float, list]:
        """
        åŸºäºè‡ªç›¸å…³ + æ ·æœ¬å¯¹æ•°æƒ©ç½š + è¿åŠ¨å‘¨æœŸæ€§ + é•¿åº¦åå¥½çš„ç»¼åˆè¯„åˆ†é€‰å‘¨æœŸ
        
        Args:
            autocorr: è‡ªç›¸å…³åºåˆ—
            N: åŸå§‹å¸§æ•°ï¼ˆç”¨äºè®¡ç®—æ ·æœ¬å¯¹æ•°ï¼‰
            min_period: æœ€å°å‘¨æœŸ
            max_period: æœ€å¤§å‘¨æœŸ
            motion_ac: è¿åŠ¨è‡ªç›¸å…³åºåˆ—ï¼ˆå¯é€‰ï¼‰
            min_pairs: æœ€å°æˆå¯¹æ ·æœ¬æ•°è¦æ±‚
            prefer_longer: æ˜¯å¦ä¼˜å…ˆé€‰æ‹©é•¿å‘¨æœŸ
            length_bias: é•¿åº¦åå¥½å¼ºåº¦
            motion_weight: è¿åŠ¨å‘¨æœŸæ€§æƒé‡
            
        Returns:
            (period, confidence, candidates)
        """
        kmin = int(min_period)
        kmax = int(min(max_period, len(autocorr) - 1))
        
        if kmax < kmin:
            return kmin, 0.0, []
        
        # æå–æœç´¢èŒƒå›´
        a = autocorr[kmin:kmax+1]  # (K,)
        
        if len(a) == 0:
            return kmin, 0.0, []
        
        # å¹³æ»‘å¤„ç†
        kernel_size = 5
        kernel = torch.ones(kernel_size, device=a.device) / kernel_size
        a_sm = F.conv1d(
            a.unsqueeze(0).unsqueeze(0),
            kernel.unsqueeze(0).unsqueeze(0),
            padding=kernel_size // 2
        ).squeeze()
        
        # å®é™…çš„ lag å€¼
        ks = torch.arange(kmin, kmax+1, device=a.device)
        
        # æ ·æœ¬å¯¹æ•°æƒ©ç½šï¼šN-k è¶Šå°ï¼Œç»Ÿè®¡è¶Šä¸ç¨³å®š
        pairs = (N - ks).clamp(min=1)
        pair_fac = torch.sqrt(pairs / pairs.max())  # [0,1]
        
        # åŸºç¡€åˆ†æ•° = å¹³æ»‘è‡ªç›¸å…³ Ã— æ ·æœ¬å¯¹æƒ©ç½š
        score = a_sm * pair_fac
        
        # èåˆè¿åŠ¨å‘¨æœŸæ€§ï¼ˆå¯¹"ç»†å¾®æŠ–åŠ¨"å‘¨æœŸé™æƒï¼‰
        if motion_ac is not None and len(motion_ac) > 1:
            # æå–å¯¹åº”çš„è¿åŠ¨è‡ªç›¸å…³åˆ‡ç‰‡
            m_start = max(0, kmin - 1)
            m_end = min(len(motion_ac), kmax)
            m_slice = motion_ac[m_start:m_end] if m_end > m_start else None
            
            if m_slice is not None and m_slice.numel() == score.numel():
                # å½’ä¸€åŒ–åˆ° [0,1]
                m_min = m_slice.min()
                m_max = m_slice.max()
                m_norm = (m_slice - m_min) / (m_max - m_min + 1e-9)
                
                # æ··åˆåˆ†æ•°
                score = (1.0 - motion_weight) * score + motion_weight * m_norm
        
        # è½»åº¦é•¿åº¦åå¥½ï¼ˆé¿å…8å¸§å¾®å¾ªç¯æŠ¢å³°ï¼‰
        if prefer_longer and (kmax - kmin) > 1 and length_bias > 0.0:
            len_norm = (ks - kmin).float() / max(1, (kmax - kmin))
            score = score + length_bias * len_norm
        
        # è®¡ç®—ç¨³å¥ç½®ä¿¡åº¦ï¼ˆä½¿ç”¨ä¸­ä½æ•°å’Œ IQRï¼‰
        med = score.median()
        q75_idx = int(0.75 * score.numel())
        q25_idx = int(0.25 * score.numel())
        q75 = score.kthvalue(min(q75_idx, score.numel())).values
        q25 = score.kthvalue(max(1, q25_idx)).values
        iqr = q75 - q25 + 1e-9
        
        # è¿‡æ»¤æˆå¯¹æ ·æœ¬å¤ªå°‘çš„å‘¨æœŸ
        valid = (pairs >= min_pairs)
        score = torch.where(valid, score, torch.full_like(score, -1e9))
        
        # æ‰¾æ‰€æœ‰å±€éƒ¨å³°å€¼
        peaks = []
        s = score
        for i in range(1, s.numel() - 1):
            if s[i] > s[i-1] and s[i] > s[i+1]:
                # ç¨³å¥æ˜¾è‘—æ€§
                prom = float((s[i] - med) / iqr)
                if prom > 0.0:
                    peaks.append({
                        "k": int(ks[i].item()),
                        "score": float(s[i].item()),
                        "prom": float(prom)
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å³°å€¼ï¼Œé€€åŒ–åˆ°å…¨å±€æœ€å¤§
        if not peaks:
            i = int(torch.argmax(s))
            k_sel = int(ks[i].item())
            conf = float((s[i] - med) / iqr)
            return k_sel, conf, [{"period": k_sel, "score": float(s[i].item()), "prominence": conf}]
        
        # æœ€é•¿ä¼˜å…ˆ within Î”ï¼ˆåœ¨åˆ†æ•°æ¥è¿‘çš„å€™é€‰ä¸­é€‰æœ€é•¿çš„ï¼‰
        peaks.sort(key=lambda x: x["score"], reverse=True)
        s_max = peaks[0]["score"]
        margin = 0.05 * max(1.0, abs(s_max))
        
        # ä»é•¿åˆ°çŸ­æ’åº
        peaks_sorted = sorted(peaks, key=lambda x: x["k"], reverse=True)
        
        chosen = None
        for p in peaks_sorted:
            if (s_max - p["score"]) <= margin:
                chosen = p
                break
        
        if chosen is None:
            chosen = peaks[0]
        
        k_sel = chosen["k"]
        conf = chosen["prom"]  # ç¨³å¥ç½®ä¿¡åº¦
        
        # åŒ…è£…å€™é€‰åˆ—è¡¨
        cands = [{"period": p["k"], "score": p["score"], "prominence": p["prom"]} for p in peaks[:5]]
        
        return k_sel, float(conf), cands
    
    def _find_best_start(
        self, 
        frames: torch.Tensor, 
        period: int, 
        device: torch.device,
        window: int = 3
    ) -> int:
        """
        é€šè¿‡æ¯”è¾ƒæ¥ç¼ç›¸ä¼¼åº¦æ‰¾åˆ°æœ€ä½³èµ·å§‹ç‚¹
        """
        N = frames.shape[0]
        
        if period >= N:
            return 0
        
        # æå–ç®€å•ç‰¹å¾ç”¨äºå¿«é€Ÿæ¯”è¾ƒ
        frames_device = frames.to(device)
        
        # è½¬ç°åº¦
        if frames.shape[-1] >= 3:
            gray = (0.299 * frames_device[..., 0] + 
                   0.587 * frames_device[..., 1] + 
                   0.114 * frames_device[..., 2])
        else:
            gray = frames_device[..., 0]
        
        # ä¸‹é‡‡æ ·åŠ é€Ÿ
        gray_small = F.interpolate(
            gray.unsqueeze(1),  # (N, 1, H, W)
            size=(64, 64),
            mode='bilinear',
            align_corners=False
        ).squeeze(1)  # (N, H, W)
        
        best_start = 0
        best_score = -float('inf')
        
        # åœ¨ [0, period) èŒƒå›´å†…æœç´¢
        for start in range(period):
            if start + period >= N:
                break
            
            # è®¡ç®—æ¥ç¼çª—å£çš„ç›¸ä¼¼åº¦
            score = 0.0
            count = 0
            
            for offset in range(-window, window + 1):
                idx1 = start + offset
                idx2 = start + period + offset
                
                if 0 <= idx1 < N and 0 <= idx2 < N:
                    # L1 è·ç¦»ï¼ˆè¶Šå°è¶Šå¥½ï¼Œå–è´Ÿå€¼ï¼‰
                    diff = (gray_small[idx1] - gray_small[idx2]).abs().mean()
                    score -= float(diff)
                    count += 1
            
            if count > 0:
                score /= count
                
                if score > best_score:
                    best_score = score
                    best_start = start
        
        return best_start
    
    def _refine_boundaries(
        self,
        frames: torch.Tensor,
        start: int,
        period: int,
        device: torch.device,
        radius: int = 4
    ) -> Tuple[int, int]:
        """
        åœ¨å°èŒƒå›´å†…å¾®è°ƒèµ·ç‚¹å’Œå‘¨æœŸ
        """
        N = frames.shape[0]
        
        if start + period >= N or radius == 0:
            return start, period
        
        frames_device = frames.to(device)
        
        # è½¬ç°åº¦å¹¶ä¸‹é‡‡æ ·
        if frames.shape[-1] >= 3:
            gray = (0.299 * frames_device[..., 0] + 
                   0.587 * frames_device[..., 1] + 
                   0.114 * frames_device[..., 2])
        else:
            gray = frames_device[..., 0]
        
        gray_small = F.interpolate(
            gray.unsqueeze(1),
            size=(64, 64),
            mode='bilinear',
            align_corners=False
        ).squeeze(1)
        
        best_start = start
        best_period = period
        best_sim = -float('inf')
        
        # åœ¨å°èŒƒå›´å†…æœç´¢
        for ds in range(-radius, radius + 1):
            for dp in range(-radius, radius + 1):
                new_start = start + ds
                new_period = period + dp
                
                if new_start < 0 or new_start + new_period >= N or new_period < 2:
                    continue
                
                # è®¡ç®—é¦–å°¾å¸§ç›¸ä¼¼åº¦
                first_frame = gray_small[new_start]
                last_frame = gray_small[new_start + new_period - 1]
                
                # SSIM çš„ç®€åŒ–ç‰ˆï¼šç›¸å…³ç³»æ•°
                sim = F.cosine_similarity(
                    first_frame.flatten().unsqueeze(0),
                    last_frame.flatten().unsqueeze(0),
                    dim=1
                )
                
                if sim > best_sim:
                    best_sim = sim
                    best_start = new_start
                    best_period = new_period
        
        return best_start, best_period
    
    def _extract_frames(
        self,
        frames: torch.Tensor,
        loop_start: int,
        loop_period: int
    ) -> torch.Tensor:
        """
        æå–å¾ªç¯å¸§
        """
        N = frames.shape[0]
        loop_end = min(loop_start + loop_period, N)
        loop_frames = frames[loop_start:loop_end]
        return loop_frames
    
    def _generate_report(
        self,
        total_frames: int,
        loop_start: int,
        loop_period: int,
        confidence: float,
        candidates: list
    ) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = f"""å¾ªç¯æ£€æµ‹æŠ¥å‘Š
==================
æ€»å¸§æ•°: {total_frames}
å¾ªç¯èµ·å§‹: {loop_start}
å¾ªç¯å‘¨æœŸ: {loop_period} å¸§
å¾ªç¯ç»“æŸ: {loop_start + loop_period}
ç½®ä¿¡åº¦: {confidence:.3f}

å€™é€‰å‘¨æœŸ:
"""
        for i, cand in enumerate(candidates[:3], 1):
            report += f"  {i}. å‘¨æœŸ={cand['period']}, å¾—åˆ†={cand['score']:.3f}, æ˜¾è‘—æ€§={cand['prominence']:.3f}\n"
        
        return report
    
    def _return_all_frames(
        self, 
        frames: torch.Tensor, 
        reason: str
    ) -> Tuple:
        """
        è¿”å›å…¨éƒ¨å¸§ï¼ˆæœªæ£€æµ‹åˆ°å¾ªç¯æ—¶çš„é™çº§ç­–ç•¥ï¼‰
        """
        N = frames.shape[0]
        
        report = f"""å¾ªç¯æ£€æµ‹æŠ¥å‘Š
==================
çŠ¶æ€: æœªæ£€æµ‹åˆ°å¾ªç¯
åŸå› : {reason}
æ€»å¸§æ•°: {N}
æ“ä½œ: è¿”å›å…¨éƒ¨å¸§
"""
        
        return (
            frames,  # å…¨éƒ¨å¸§ä½œä¸º loop_frames
            0,       # loop_start
            N,       # loop_period (æ•´ä¸ªè§†é¢‘)
            0.0,     # confidence
            report
        )


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "LoopDetectExtract": LoopDetectExtract,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LoopDetectExtract": "ğŸ” å¾ªç¯æ£€æµ‹ä¸æå–",
}

